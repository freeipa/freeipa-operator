# Redirect traffic to the cluster

The goal of this proof of concept is to investigate the traffic to our pods
in different scenarios:

- [x] **[Scenario 1 - Inside the pod](#scenario-1---inside-the-pod)**: A client container should be able to connect the
  different ports of the pod.
- [x] **[Scenario 2 - Intra Pod](#scenario-2---intra-pod)**: A client in a secondary pod in the same node
  should be able to connect to the different pod ports.
- [x] **[Scenario 3 - Intra Pod different Nodes](#scenario-3---intra-pod-different-nodes)**: A client in a secondary pod in a
  different node should be able to connect to the different pod ports.
- [ ] **[Scenario 4 - External access](#scenario-4---external-access)**: Allowing access from out of the cluster to the
  cluster.
  - [x] **[nodePort](#nodePort)**.
        - [x] It worked on CodeReadyContainers with no changes.
        - [x] It worked on OpenStack cluster by adding two rules to the security group.
  - [ ] **[externalIP + NodePort](#externalips--nodeport)**.
        - [x] It worked on CodeReadyContainers by assigning an extra IP to the VM.
        - [ ] Pending to run the same in an OpenStack cluster.
  - [ ] **[externalIP + LoadBalancer](#externalip--loadbalancer)**.
        - [ ] It should work with OpenStack Victoria.
- [ ] **[Scenario 5 - Intracluster](#scenario-5---intracluster)**: A natural extension from the above is using the
  services from another cluster so that we close the circle for the hybrid cloud scenario in terms of network traffic.
      - [ ] Deploy in cluster 1 and use the services from cluster 2.
      - [ ] Deploy in cluster 1 and cluster 2; cluster 2 is federated to cluster 1.

The main pod is a set of containers running an echo server using different
ports and transport layer, simulating the network infrastructure that
create Freeipa. Here the focus is the connectivity regarding to infrastructure
rather than the specific protocol is working on the port and transport layer.

## Playing locally

```shell
export DOCKER_IMAGE=docker.io/echo/server:1.0.0
# For running UDP server
make run-udp ECHO_PORT=8007 ECHO_ADDRESS="0.0.0.0"
# For running TCP server
make run-tcp ECHO_PORT=8007 ECHO_ADDRESS="0.0.0.0" ECHO_MAX_CLIENTS=1
```

> The example above is using the default values so we can just run it
> without overriding variables by just `make run-udp` and `make run-tcp`.

And from another terminal:

```shell
# For UDP server
echo "hello" | nc -4u -w1 localhost 8007  # Makefile set default port to 8007
# For TCP server
echo "hello" | nc -4 -w1 localhost 8007   # Makefile set default port to 8007
```

## Preparing the environment

This proof of concept need a container image generated by
[this Dockerfile](Dockerfile), which build the echo server image
to be used on the different scenarios.

```shell
oc login "${MY_CLUSTER_API}"
# Change DOCKER_IMAGE to your registry, but keep in mind
# that should be changed on the manifest files too.
export DOCKER_IMAGE=quay.io/freeipa/freeipa-operator:poc-004
podman login "${DOCKER_IMAGE##/*}"
make build push app-deploy
make get-info
```

## Scenario 1 - Inside the pod

In this scenario we try to deploy one pod with all the services, and we launch
a container inside the pod for testing the connectivity inside the pod.

Deploy the PoC by:

```shell
APP=poc-004-1 make app-deploy
```

The following containers are created into the pod:

- **client**: It is used to execute the checkings.
- **http**: Echo service listening at port 80.
- **https**: Echo service listening at port 443.
- **ldap**: Echo service listening at port 389.
- **ldaps**: Echo service listening at port 636.
- **kerberos-tcp**: Echo service listening at tcp port 88.
- **kerberos-admin-tcp**: Echo service listening at tcp port 464.
- **kerberos-udp**: Echo service listening at udp port 88.
- **kerberos-admin-udp**: Echo service listening at udp port 464.

The deployment look like the below:

```raw
# oc get pods -o wide
NAME        READY   STATUS    RESTARTS   AGE   IP             NODE                              NOMINATED NODE   READINESS GATES
poc-004-1   9/9     Running   0          46m   10.140.3.212   avisiedo20-lklfq-worker-0-44tdb   <none>           <none>
```

Trying the connectivity inside the Pod. This launch from the client
container a challenge to reach the ports and compare the response or
report timeout if that were the case, indicating a report of what ports
were reached properly:

```shell
./check-scenario-1.sh
```

Finally, we release the resources allocated for the proof of concept by:

```shell
APP=poc-004-1 make app-delete
```

## Scenario 2 - Intra Pod

Deploy the PoC by:

```shell
APP=poc-004-2 make app-deploy
```

Here is important the section in the poc-004-2-client pod by using the below:

```yaml
  affinity:
    podAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        - topologyKey: "kubernetes.io/hostname"
          labelSelector:
            matchLabels:
              app: poc-004-2
              role: services
```

Note that we are using only in one pod. You could be tempted to append to both
pods, but if we did that none of the pods would be scheduled and they will
never be deployed. This is because an interlook evoked by the conditions, so in
any case none of the pods has been scheduled yet when the condition is checked
to schedule them, so the condition is always false and fails the scheduling
process for ever.

Setting the condition only in one pod, it could fails a few times, but once the
other pod that the condition is checked on, the pod is scheduled properly
applying the affinity criteria.

Once the pods are deployed we can check that both pods are in the same node by:

```raw
oc get pods -o wide
NAME                 READY   STATUS    RESTARTS   AGE   IP             NODE                           NOMINATED NODE   READINESS GATES
poc-004-2-client     1/1     Running   0          23s   10.141.2.220   permanent-bdd7p-worker-9r4b6   <none>           <none>
poc-004-2-services   8/8     Running   0          22s   10.141.2.221   permanent-bdd7p-worker-9r4b6   <none>           <none>
```

Trying the connectivity between the Pods:

```shell
./check-scenario-2.sh
```

Delete the PoC:

```shell
APP=poc-004-2 make app-delete
```

## Scenario 3 - Intra Pod different Nodes

Deploy the poc by:

```shell
APP=poc-004-3 make app-deploy
```

Similar deployment to the above, but we instead of `podAffinity` we now use
`podAntiAffinity` to get the pods deployed in different nodes:

```yaml
  affinity:
    podAntiAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        - topologyKey: "kubernetes.io/hostname"
          labelSelector:
            matchLabels:
              app: poc-004-3
              role: services
```

And now we double check the pods are deployed in different nodes:

```raw
# oc get pods -o wide
NAME                 READY   STATUS    RESTARTS   AGE   IP             NODE                           NOMINATED NODE   READINESS GATES
poc-004-3-client     1/1     Running   0          81s   10.141.2.220   permanent-bdd7p-worker-9r4b6   <none>           <none>
poc-004-3-services   9/9     Running   0          80s   10.143.0.205   permanent-bdd7p-worker-x7cmm   <none>           <none>
```

Execute the checking script:

```shell
./check-scenario-3.sh
```

And finally we delete the resources allocated for the PoC by:

```shell
APP=poc-004-3 make app-delete
```

## Scenario 4 - External access

Different options have been investigated for the external access:

- **NodePort**: This needs some hacks in OpenStack by adding some changes to the
  security groups.
- **externalIPs + NodePort**: This investigation is moved to a new ticket
  to avoid this ticket grow too much.
- **externalIPs + LoadBalancer**: This investigation is moved to a new ticket
  to avoid this ticket grow too much.

### nodePort

[poc-004-4c.yaml](poc-004-4c.yaml).

> You need to modify the security group of your OpenStack infrastructure
> to allow communication to the NodePorts indicated into the manifest files.
> One problem detected here is that the rules created by default doesn't
> allow to route the traffic to inside the cluster because the remote prefix
> should be `0.0.0.0/0` instead of `10.8.0.0/16`.

> It can be made by:

```shell
# 24114e71-7523-42c2-a48e-16b055600351  is the security group id
# You can retrieve it by:
#   openstack security group list
openstack security group rule create --ingress --ethertype IPv4 --dst-port 30000:32767 --protocol udp --remote-ip 0.0.0.0/0 24114e71-7523-42c2-a48e-16b055600351
openstack security group rule create --ingress --ethertype IPv4 --dst-port 30000:32767 --protocol tcp --remote-ip 0.0.0.0/0 24114e71-7523-42c2-a48e-16b055600351
```

We can see the rules as the below in yaml format:

```yaml
# openstack security group show <cluster-infra-id>-worker
# they can be created by:
rules: created_at=''2020-11-17T19:45:20Z'', direction=''ingress'', ethertype=''IPv4'',
  id=''bf4db301-ea8d-4cc9-bb66-69f0b012e201'', port_range_max=''32767'', port_range_min=''30000'',
  protocol=''udp'', remote_ip_prefix=''0.0.0.0/0'', updated_at=''2020-11-17T19:45:20Z''

  created_at=''2020-11-17T19:44:43Z'', direction=''ingress'', ethertype=''IPv4'',
  id=''7d561c9b-b80c-4bc2-b32c-d09f99d05012'', port_range_max=''32767'', port_range_min=''30000'',
  protocol=''tcp'', remote_ip_prefix=''0.0.0.0/0'', updated_at=''2020-11-17T19:44:43Z''
```

Options to this situation:

- Add the rule by hand in OpenStack after the OpenShift cluster has been
  deployed.

- idm-ocp-provisioner could create a security group and add it as an
  additional security group in the `install-config.yaml`:

  ```yaml
  compute:
  - name: worker
    platform:
      openstack:
        additionalSecurityGroupIDs:
        - 7ee219f3-d2e9-48a1-96c2-e7429f1b0da7
  ```

Now we can see that the ports are opened, after deploying the manifest:

```raw
# APP=poc-004-4c make app-deploy
# nmap 10.0.135.189 -p 31080,31443,31389,31636,31088,31464
Starting Nmap 7.80 ( https://nmap.org ) at 2020-11-17 21:47 CET
Nmap scan report for poc-05-c.apps.permanent.idmocp.lab.eng.rdu2.redhat.com (10.0.135.189)
Host is up (0.17s latency).

PORT      STATE SERVICE
31080/tcp open  unknown
31088/tcp open  unknown
31389/tcp open  unknown
31443/tcp open  unknown
31464/tcp open  unknown
31636/tcp open  unknown

Nmap done: 1 IP address (1 host up) scanned in 0.36 seconds
```

We can play with the proof of concept by:

```shell
APP=poc-004-4c make app-deploy
./check-scenario-4c.sh
APP=poc-004-4c make app-delete
```

A failing output would be the below:

```raw
Ncat: TIMEOUT.
Port 31080/tcp: >> Failed
Ncat: TIMEOUT.
Port 31443/tcp: >> Failed
Ncat: TIMEOUT.
Port 31389/tcp: >> Failed
Ncat: TIMEOUT.
Port 31636/tcp: >> Failed
Ncat: TIMEOUT.
Port 31088/tcp: >> Failed
Ncat: TIMEOUT.
Port 31464/tcp: >> Failed
Port 31088/udp: >> Failed
Port 31464/udp: >> Failed
oc logs pod/poc-004-4c -c http --tail 7
INFO:root:Listening on 0.0.0.0:80/tcp
INFO:root:You can test the connection by: nc localhost 80
INFO:root:Waiting for connection
oc logs pod/poc-004-4c -c https --tail 7
INFO:root:Listening on 0.0.0.0:443/tcp
INFO:root:You can test the connection by: nc localhost 443
INFO:root:Waiting for connection
oc logs pod/poc-004-4c -c ldap --tail 7
INFO:root:Listening on 0.0.0.0:389/tcp
INFO:root:You can test the connection by: nc localhost 389
INFO:root:Waiting for connection
oc logs pod/poc-004-4c -c ldaps --tail 7
INFO:root:Listening on 0.0.0.0:636/tcp
INFO:root:You can test the connection by: nc localhost 636
INFO:root:Waiting for connection
oc logs pod/poc-004-4c -c kerberos-tcp --tail 7
INFO:root:Listening on 0.0.0.0:88/tcp
INFO:root:You can test the connection by: nc localhost 88
INFO:root:Waiting for connection
oc logs pod/poc-004-4c -c kerberos-admin-tcp --tail 7
INFO:root:Listening on 0.0.0.0:464/tcp
INFO:root:You can test the connection by: nc localhost 464
INFO:root:Waiting for connection
oc logs pod/poc-004-4c -c kerberos-udp --tail 7
INFO:root:Listening on 0.0.0.0:88/udp
INFO:root:You can test the connection by: nc -4u localhost 88
oc logs pod/poc-004-4c -c kerberos-admin-udp --tail 7
INFO:root:Listening on 0.0.0.0:464/udp
INFO:root:You can test the connection by: nc -4u localhost 464
```

And a successful output would be the below:

```raw
Port 31080/tcp: Success
Port 31443/tcp: Success
Port 31389/tcp: Success
Port 31636/tcp: Success
Port 31088/tcp: Success
Port 31464/tcp: Success
Port 31088/udp: Success
Port 31464/udp: >> Failed   # this can happen
oc logs pod/poc-004-4c -c http --tail 7
INFO:root:Waiting for connection
INFO:root:Dispatching client 10.143.0.1:34559
INFO:root:Connection from ('10.143.0.1', 34559)
INFO:root:<< 'hello world'
INFO:root:Waiting for connection
INFO:root:<< ''
INFO:root:No more data from ('10.143.0.1', 34559)
oc logs pod/poc-004-4c -c https --tail 7
INFO:root:Waiting for connection
INFO:root:Dispatching client 10.143.0.1:41485
INFO:root:Connection from ('10.143.0.1', 41485)
INFO:root:Waiting for connection
INFO:root:<< 'hello world'
INFO:root:<< ''
INFO:root:No more data from ('10.143.0.1', 41485)
oc logs pod/poc-004-4c -c ldap --tail 7
INFO:root:Waiting for connection
INFO:root:Dispatching client 10.143.0.1:29753
INFO:root:Connection from ('10.143.0.1', 29753)
INFO:root:<< 'hello world'
INFO:root:Waiting for connection
INFO:root:<< ''
INFO:root:No more data from ('10.143.0.1', 29753)
oc logs pod/poc-004-4c -c ldaps --tail 7
INFO:root:Waiting for connection
INFO:root:Dispatching client 10.143.0.1:17829
INFO:root:Connection from ('10.143.0.1', 17829)
INFO:root:Waiting for connection
INFO:root:<< 'hello world'
INFO:root:<< ''
INFO:root:No more data from ('10.143.0.1', 17829)
oc logs pod/poc-004-4c -c kerberos-tcp --tail 7
INFO:root:Waiting for connection
INFO:root:Dispatching client 10.143.0.1:58835
INFO:root:Connection from ('10.143.0.1', 58835)
INFO:root:Waiting for connection
INFO:root:<< 'hello world'
INFO:root:<< ''
INFO:root:No more data from ('10.143.0.1', 58835)
oc logs pod/poc-004-4c -c kerberos-admin-tcp --tail 7
INFO:root:Waiting for connection
INFO:root:Dispatching client 10.143.0.1:30818
INFO:root:Connection from ('10.143.0.1', 30818)
INFO:root:Waiting for connection
INFO:root:<< 'hello world'
INFO:root:<< ''
INFO:root:No more data from ('10.143.0.1', 30818)
oc logs pod/poc-004-4c -c kerberos-udp --tail 7
INFO:root:Listening on 0.0.0.0:88/udp
INFO:root:You can test the connection by: nc -4u localhost 88
INFO:root:<< 'hello world'
oc logs pod/poc-004-4c -c kerberos-admin-udp --tail 7
INFO:root:Listening on 0.0.0.0:464/udp
INFO:root:You can test the connection by: nc -4u localhost 464
```

#### References

- https://kubernetes.io/docs/concepts/services-networking/service/#publishing-services-service-types
- https://kubernetes.io/docs/concepts/services-networking/service/#nodeport

### externalIPS + NodePort

[poc-004-4a.yaml](poc-004-4a.yaml).

Using Services with externalIPs assigned to the worker nodes so that
the traffic is routed to it. This require to assign an additional IP to the
cluster. Furthermore, this require add a DNS for resolving the additional IP.
This option could allow to use a different subdomain to the one used by the
cluster.

```raw

   freeipa.redhat.com > 10.0.135.15
    |      api.cluster.redhat.com > 10.0.135.95
    |       |     *.apps.cluster.redhat.com > 10.0.135.189
    |       |        |
    v       v        v
 +-----------------------+
 |  My Cluster           |
 +-----------------------+
```

- **Upsides**:

  - We can route no http ports to the cluster with no conflicts
    with service ports (because of the externalIP).

- **Downsides**:

  - As it has been read in some articles, manual configuration is needed; in
    our scenario, it has been tried on OpenStack platform but no other
    infrastructures. At the moment of writing this lines, I don't know how to
    do this manual set up into the infrastructure, some help from
    OpenStack/OpenShift expert is needed here. It is used OpenStack Queens.

    ```raw
    # curl -s https://rhos-d.infra.prod.upshift.rdu2.redhat.com:13000/v3 | jq -r '.[] | .id'
    v3.10
    ```

  - Use of ExternalIP functionality can be a security risk, because
    in-cluster traffic to an external IP address is directed to that Service.
    This could allow cluster users to intercept sensitive traffic destined
    for external resources. So that the 80 and 389 ports, which are not
    encrypted, must be rejected.

  - For the port 443, we lose the functionality provided by Route object
    specific for OpenShift; but we can fix that creating a different service
    for that port, and bind it with a Route object.

Some manual configuration is needed to enable this feature which is disabled by default
in OpenShift (using v4.4).

- Modify **network/cluster** to accept the external IPs (you need to use an
  admin account).

  ```raw
  # oc patch network/cluster --type=merge -p '{"spec":{"externalIP":{"autoAssignCIDRs":["10.0.132.0/22"],"policy":null}}}'
  ```

- Even with the above further configuration is needed on OpenStack that
  I don't know yet how to set it up.

---

Play with it by:

```shell
APP=poc-004-4a make app-delete app-deploy
echo -n "hello" | nc -4 10.0.133.188 88
# But the ip is not assigned, so no traffic arrives
```

```raw
# oc get services
NAME             TYPE        CLUSTER-IP       EXTERNAL-IP    PORT(S)                          AGE
poc-004-4a-tcp   ClusterIP   172.30.103.198   10.0.133.188   389/TCP,636/TCP,88/TCP,464/TCP   9m8s
poc-004-4a-udp   ClusterIP   172.30.182.193   10.0.133.188   88/UDP,464/UDP                   9m7s
poc-004-4a-web   ClusterIP   172.30.204.89    10.0.133.188   80/TCP,443/TCP                   9m7s
```

#### Questions

- Is it possible to set up OpenShift to allocate automatically the floating ips
  for the externalIPs when they are needed? If so, how?
- Could OpenShift be set up for auto-assign external IPs based on a pool of
  IP list instead of the CIDR format? How? exist any annotation for this?
- How should OpenStack be set up to add more than one public IP to a trunk?
  It is not possible to assing more than one floating IP into a trunk.
  Should a bridge be assigned to the trunk port? How to do that? I didn't
  have a response to this.

> A barrier has been found here related with how to set up
> OpenStack to assign more than one IP address for a trunk
> object in OpenStack, so that the traffic could be routed
> to the worker nodes properly.
>
> This investigation is in stand by so far.

Some task of interest:

- Manual allocation for a floating IP in OpenStack:

  ```raw
  source env.sh
  source openrc.sh
  INFRA_ID="$( cat clusters/permanent/infra_id.txt )"
  openstack floating ip create --description "${INFRA_ID}-ingress" --project "${OS_PROJECT_ID}" "${NETWORK}"
  ```

### externalIP + LoadBalancer

[poc-004-4b.yaml](poc-004-4b.yaml).

This configuration has some objections:

> Depending on your RHOSP environment, Octavia might not support UDP
> listeners. If you use Kuryr SDN on RHOSP version 15 or earlier, UDP
> services are not supported. RHOSP version 16 or later support UDP.
>
> In our scenario we are using RHOSP version 13, so we can not test UDP
> listener.

For using OVN driver:

> Beginning with RHOSP version 16, the Octavia OVN provider driver (ovn)
> is supported on OpenShift Container Platform on RHOSP deployments.
>
> In our scenario we are using RHOSP version 13, so we can not test
> the load balancer using Octavia OVN.

On this scenario, the expectation was to using a Service.spec.type=LoadBalancer
for routing the traffic that arrive to the cluster and distribute it between
several components. This option will be helpful when deploying several
replicas.

Unfortunatly, this functionality need support from the cloud provider which
is OpenStack in our case. It was checked if openstack has support for octavia:

```shell
openstack loadbalancer provider list
```

Some additional notes about this:

- The `permanent` cluster is deployed with `octaviaSupport: false` so no
  support for load balancer is provided. It has been tried to deploy a
  new cluster OpenShift 4.4 enabling that option, but the cluster
  was not accesible when the process is finished. It was tested deploying
  an OpenShift 4.5 cluster too without a successful scenario. Some notes here:

  - Something is wrong in the container that avoid
    openshift-install set up properly the LoadBalancer support.
    I tried to modify the container using the exact versions of
    openstack clients and client plugins that match with the OpenStack
    service, but it didn't work, so probably this is not the cause.
    By the way I would like to double check this with some expert
    on OpenStack.

  - Some OpenStack support need to be enabled for this. I don't know
    what could be. Permissions? some additional actions? Again, I
    would need to double check this with some expert on OpenStack.

  - Something is wrong in OpenShift configuration that avoid the load
    balancer to be instanciated.

- Additionally a public IP is assigned for the LoadBalancer so this
  will need to assign extra IPs to the cluster, so that we go back
  to the scenario of the externalIP too.

---

You can play with the PoC by:

```shell
APP=poc-004-4b make app-delete app-deploy
oc get all
nc -4 external-ip 88
```

When deploying in bare metal, an alternative could be:

- [MetalLB](https://metallb.universe.tf/). Not tested sill this scenario.
  More information at [Install MetalLB](https://metallb.universe.tf/installation/)
  and [Install Keeplived operator](https://github.com/redhat-cop/keepalived-operator#deploying-the-operator).

More information can be found in the article below:

- [Self-hosted Load Balancer for OpenShift: an Operator Based Approach](https://www.openshift.com/blog/self-hosted-load-balancer-for-openshift-an-operator-based-approach).

For installing keepalived, just do the below:

```shell
git clone https://github.com/redhat-cop/keepalived-operator.git
cd keepalived-operator
oc apply -f deploy/crds/redhatcop.redhat.io_keepalivedgroups_crd.yaml
oc new-project keepalived-operator
oc -n keepalived-operator apply -f deploy
```

## Scenario 5 - Intracluster

- [externalName](https://kubernetes.io/docs/concepts/services-networking/service/#externalname).

TODO

## References

- [externalIPS](https://kubernetes.io/docs/concepts/services-networking/service/#external-ips).
- [Learn how to use Kubernetes External IP service type](https://medium.com/swlh/kubernetes-external-ip-service-type-5e5e9ad62fcd).
- [Configuring Ingress cluster traffic service with external IP](https://docs.openshift.com/container-platform/4.4/networking/configuring_ingress_cluster_traffic/configuring-ingress-cluster-traffic-service-external-ip.html#nw-service-externalip-create_configuring-ingress-cluster-traffic-service-external-ip).
- [Service.Type=NodePort](https://kubernetes.io/docs/concepts/services-networking/service/#nodeport).
- [Service.Type=LoadBalancer](https://kubernetes.io/docs/concepts/services-networking/service/#loadbalancer).
- [Assigning egress IPs](https://docs.openshift.com/container-platform/4.4/networking/openshift_sdn/assigning-egress-ips.html).
- [Configuring External IP](https://docs.openshift.com/container-platform/4.4/networking/configuring_ingress_cluster_traffic/configuring-externalip.html).
- [Openshift and CNV - Part 2 - Exposing Virtualized Services](https://blog.oddbit.com/post/2020-07-30-openshift-and-cnv-part-2-expos/).
- [Kubernetes services networking](https://docs.openstack.org/kuryr-kubernetes/latest/installation/services.html).
- [OpenShift Container Platform on Red Hat OpenStack Platform Support Matrix](https://access.redhat.com/articles/4679401).
- [Installing a cluster on OpenStack with Kuryr](https://docs.openshift.com/container-platform/4.4/installing/installing_openstack/installing-openstack-installer-kuryr.html).
- [Openshift 4 Bare Metal install - Quickstart](https://www.openshift.com/blog/openshift-4-bare-metal-install-quickstart).
- [Accessing CodeReady Containers on a Remote Server](https://www.openshift.com/blog/accessing-codeready-containers-on-a-remote-server/).
- [KVM/libvirt: How to configure static guest IP addresses on the virtualisation host](https://serverfault.com/questions/627238/kvm-libvirt-how-to-configure-static-guest-ip-addresses-on-the-virtualisation-ho).
- [Chapter 27. Assigning Unique External IPs for Ingress Traffic](https://access.redhat.com/documentation/en-us/openshift_container_platform/3.10/html/cluster_administration/admin-guide-unique-external-ips-ingress-traffic).
- [Chapter 20. Load balancing on RHOSP](https://access.redhat.com/documentation/en-us/openshift_container_platform/4.6/html/networking/load-balancing-openstack).
